{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "\n",
    "class SimIFAModule(nn.Module):\n",
    "    def __init__(self, in_channels=128, out_channels=[1024, 512, 256, 128]):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.rand((in_channels, sum(out_channels), 1, 1)))\n",
    "\n",
    "    def forward(self, p5, p4, p3, p2):\n",
    "        # torch.cuda.synchronize()\n",
    "        x5 = F.interpolate(p5, scale_factor=8, align_corners=False, mode='bilinear')\n",
    "        x4 = F.interpolate(p4, scale_factor=4, align_corners=False, mode='bilinear')\n",
    "        x3 = F.interpolate(p3, scale_factor=2, align_corners=False, mode='bilinear')\n",
    "        x2 = p2\n",
    "        x_fuse = torch.concat([x5, x4, x3, x2], dim=1)\n",
    "        # x_fuse: [1, 1920, 256, 256]\n",
    "        output = F.conv2d(x_fuse, self.weights)\n",
    "        # torch.cuda.synchronize()\n",
    "        return output\n",
    "\n",
    "\n",
    "class SimCFAModuleForFlops(nn.Module):\n",
    "    def __init__(self, in_channels=128, out_channels=[1024, 512, 256, 128]):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.rand((in_channels, sum(out_channels), 1, 1)))\n",
    "        self.conv = nn.Conv3d(4, 1, 1, bias=False)\n",
    "        nn.init.constant_(self.conv.weight, 1)\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, p5, p4, p3, p2):\n",
    "        x5 = F.interpolate(F.conv2d(p5, self.weights[:, :self.out_channels[0]]), scale_factor=8, align_corners=False, mode='bilinear')\n",
    "        x4 = F.interpolate(F.conv2d(p4, self.weights[:, sum(self.out_channels[:1]):sum(self.out_channels[:2])]), scale_factor=4, align_corners=False, mode='bilinear')\n",
    "        x3 = F.interpolate(F.conv2d(p3, self.weights[:, sum(self.out_channels[:2]):sum(self.out_channels[:3])]), scale_factor=2, align_corners=False, mode='bilinear')\n",
    "        x2 = F.conv2d(p2, self.weights[:, sum(self.out_channels[:3]):])\n",
    "        concat = torch.concat([x5.unsqueeze(0), x4.unsqueeze(0), x3.unsqueeze(0), x2.unsqueeze(0)], dim=0)\n",
    "        concat = concat.permute(1, 0, 2, 3, 4)\n",
    "        output = self.conv(concat)\n",
    "        # output = x5 + x4 + x3 + x2\n",
    "        # output = torch.add([x5, x4, x3, x2])\n",
    "        return output\n",
    "\n",
    "\n",
    "class SimCFAModule(nn.Module):\n",
    "    def __init__(self, in_channels=128, out_channels=[1024, 512, 256, 128]):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.rand((in_channels, sum(out_channels), 1, 1)))\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, p5, p4, p3, p2):\n",
    "        # torch.cuda.synchronize()\n",
    "        x5 = F.interpolate(F.conv2d(p5, self.weights[:, :self.out_channels[0]]), scale_factor=8, align_corners=False, mode='bilinear')\n",
    "        x4 = F.interpolate(F.conv2d(p4, self.weights[:, sum(self.out_channels[:1]):sum(self.out_channels[:2])]), scale_factor=4, align_corners=False, mode='bilinear')\n",
    "        x3 = F.interpolate(F.conv2d(p3, self.weights[:, sum(self.out_channels[:2]):sum(self.out_channels[:3])]), scale_factor=2, align_corners=False, mode='bilinear')\n",
    "        x2 = F.conv2d(p2, self.weights[:, sum(self.out_channels[:3]):])\n",
    "        output = x5 + x4 + x3 + x2\n",
    "        # torch.cuda.synchronize()\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLOPs\n",
    "inputs = (\n",
    "    torch.rand((1, 1024, 32, 32)),\n",
    "    torch.rand((1, 512, 64, 64)),\n",
    "    torch.rand((1, 256, 128, 128)),\n",
    "    torch.rand((1, 128, 256, 256)),\n",
    ")\n",
    "\n",
    "IFA = SimIFAModule()\n",
    "CFA = SimCFAModuleForFlops()\n",
    "\n",
    "print()\n",
    "flops = FlopCountAnalysis(IFA, inputs)\n",
    "print(\"IFA flops counter: \")\n",
    "print(flops.total())\n",
    "print(flops.by_operator())\n",
    "\n",
    "print()\n",
    "flops = FlopCountAnalysis(CFA, inputs)\n",
    "print(\"CFA flops counter: \")\n",
    "print(flops.total())\n",
    "print(flops.by_operator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "      aten::cudnn_convolution        20.78%     446.000us        43.57%     935.000us     233.750us     814.000us        42.82%       1.138ms     284.500us             4  \n",
      "        cudaDeviceSynchronize        16.22%     348.000us        16.22%     348.000us     348.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "             cudaEventDestroy        14.86%     319.000us        14.86%     319.000us       2.185us       0.000us         0.00%       0.000us       0.000us           146  \n",
      "              cudaEventRecord        13.98%     300.000us        13.98%     300.000us       2.000us       0.000us         0.00%       0.000us       0.000us           150  \n",
      "             cudaLaunchKernel         5.17%     111.000us         5.17%     111.000us       5.045us       0.000us         0.00%       0.000us       0.000us            22  \n",
      "                  aten::slice         4.01%      86.000us         9.51%     204.000us      25.500us      63.000us         3.31%      79.000us       9.875us             8  \n",
      "    aten::upsample_bilinear2d         3.63%      78.000us         8.85%     190.000us      31.667us     235.000us        12.36%     448.000us      74.667us             6  \n",
      "                  aten::empty         2.89%      62.000us         4.38%      94.000us      11.750us      55.000us         2.89%      55.000us       6.875us             8  \n",
      "                  aten::copy_         2.75%      59.000us         4.85%     104.000us      26.000us      86.000us         4.52%      86.000us      21.500us             4  \n",
      "                    aten::add         2.24%      48.000us         3.45%      74.000us      24.667us     366.000us        19.25%     366.000us     122.000us             3  \n",
      "             aten::as_strided         2.19%      47.000us         3.73%      80.000us      10.000us      16.000us         0.84%      16.000us       2.000us             8  \n",
      "           aten::_convolution         1.72%      37.000us        46.13%     990.000us     247.500us      35.000us         1.84%       1.173ms     293.250us             4  \n",
      "                aten::resize_         1.58%      34.000us         3.12%      67.000us       8.375us      31.000us         1.63%      31.000us       3.875us             8  \n",
      "                 aten::conv2d         1.40%      30.000us        50.42%       1.082ms     270.500us      24.000us         1.26%       1.221ms     305.250us             4  \n",
      "                  aten::clone         1.40%      30.000us        11.46%     246.000us      61.500us      66.000us         3.47%     227.000us      56.750us             4  \n",
      "            aten::convolution         1.35%      29.000us        48.23%       1.035ms     258.750us      24.000us         1.26%       1.197ms     299.250us             4  \n",
      "             aten::empty_like         1.30%      28.000us         4.33%      93.000us      23.250us      46.000us         2.42%      75.000us      18.750us             4  \n",
      "             aten::contiguous         1.16%      25.000us        13.42%     288.000us      72.000us      40.000us         2.10%     267.000us      66.750us             4  \n",
      "              cudaEventCreate         0.98%      21.000us         0.98%      21.000us       0.144us       0.000us         0.00%       0.000us       0.000us           146  \n",
      "        cudaFuncGetAttributes         0.37%       8.000us         0.37%       8.000us       2.667us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "         cudaFuncSetAttribute         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.146ms\n",
      "Self CUDA time total: 1.901ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Latency\n",
    "inputs = (\n",
    "    torch.rand((1, 1024, 32, 32)).cuda(),\n",
    "    torch.rand((1, 512, 64, 64)).cuda(),\n",
    "    torch.rand((1, 256, 128, 128)).cuda(),\n",
    "    torch.rand((1, 128, 256, 256)).cuda(),\n",
    ")\n",
    "\n",
    "# IFA = SimIFAModule().cuda()\n",
    "CFA = SimCFAModule().cuda()\n",
    "# IFA.weights.data = CFA.weights.data\n",
    "\n",
    "# warm up\n",
    "# ifa_output = IFA(inputs[0], inputs[1], inputs[2], inputs[3])\n",
    "cfa_output = CFA(inputs[0], inputs[1], inputs[2], inputs[3])\n",
    "# print(\"check the same output: \", (cfa_output.sum() - ifa_output.sum()))\n",
    "\n",
    "# with torch.autograd.profiler.profile(enabled=True, use_cuda=True, record_shapes=False) as prof:\n",
    "#     IFA(inputs[0], inputs[1], inputs[2], inputs[3])\n",
    "# # NOTE: some columns were removed for brevity\n",
    "# print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
    "\n",
    "with torch.autograd.profiler.profile(enabled=True, use_cuda=True, record_shapes=False) as prof:\n",
    "    CFA(inputs[0], inputs[1], inputs[2], inputs[3])\n",
    "# NOTE: some columns were removed for brevity\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('YOSO')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18d1c34aac4e50ff4dc786cc8ed509f79524cdbbbd585bbcbda27df53377b6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
